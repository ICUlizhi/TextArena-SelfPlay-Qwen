#!/usr/bin/env python3
"""
å®Œæ•´æ¨¡å‹è¯„ä¼°è„šæœ¬
å¯¹æ‰€æœ‰7ä¸ªæ¨¡å‹è¿›è¡Œ100ä¸ªæ ·ä¾‹çš„å®Œæ•´æµ‹è¯•
"""

import os
import json
import subprocess
import time
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed
import argparse

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# æ¨¡å‹é…ç½®
MODELS = [
    {
        "id": "baseline",
        "name": "QwenåŸºçº¿æ¨¡å‹",
        "path": "/mnt/cvda/cvda_avatar/1/textarena-selfplay-qwen/qwen",
        "type": "base"
    },
    {
        "id": "tiny",
        "name": "Tiny CoTæ¨¡å‹",
        "path": "/mnt/cvda/cvda_avatar/1/textarena-selfplay-qwen/models/qwen_tiny_cot_lora",
        "type": "lora"
    },
    {
        "id": "short",
        "name": "Short CoTæ¨¡å‹", 
        "path": "/mnt/cvda/cvda_avatar/1/textarena-selfplay-qwen/models/qwen_short_cot_lora",
        "type": "lora"
    },
    {
        "id": "medium",
        "name": "Medium CoTæ¨¡å‹",
        "path": "/mnt/cvda/cvda_avatar/1/textarena-selfplay-qwen/models/qwen_medium_cot_lora", 
        "type": "lora"
    },
    {
        "id": "long",
        "name": "Long CoTæ¨¡å‹",
        "path": "/mnt/cvda/cvda_avatar/1/textarena-selfplay-qwen/models/qwen_long_cot_lora",
        "type": "lora"
    },
    {
        "id": "very_long",
        "name": "Very Long CoTæ¨¡å‹",
        "path": "/mnt/cvda/cvda_avatar/1/textarena-selfplay-qwen/models/qwen_very_long_cot_lora",
        "type": "lora"
    },
    {
        "id": "ultra_long",
        "name": "Ultra Long CoTæ¨¡å‹",
        "path": "/mnt/cvda/cvda_avatar/1/textarena-selfplay-qwen/models/qwen_ultra_long_cot_lora",
        "type": "lora"
    }
]

def get_available_gpus():
    """æ£€æµ‹å¯ç”¨çš„GPU"""
    if not TORCH_AVAILABLE:
        print("âš ï¸  PyTorchä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
        return []
    
    try:
        gpu_count = torch.cuda.device_count()
        if gpu_count == 0:
            print("âš ï¸  æœªæ£€æµ‹åˆ°CUDA GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            return []
        
        available_gpus = []
        for i in range(gpu_count):
            try:
                # æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
                torch.cuda.set_device(i)
                memory_info = torch.cuda.get_device_properties(i)
                available_gpus.append({
                    "id": i,
                    "name": memory_info.name,
                    "memory": memory_info.total_memory // (1024**3)  # GB
                })
            except Exception as e:
                print(f"âš ï¸  GPU {i} ä¸å¯ç”¨: {e}")
        
        return available_gpus
    except Exception as e:
        print(f"âš ï¸  GPUæ£€æµ‹å¤±è´¥: {e}")
        return []

def assign_models_to_gpus(models, available_gpus):
    """å°†æ¨¡å‹åˆ†é…åˆ°GPU"""
    if not available_gpus:
        # æ²¡æœ‰GPUï¼Œä½¿ç”¨CPU
        return [(model, "cpu") for model in models]
    
    # å¾ªç¯åˆ†é…æ¨¡å‹åˆ°GPU
    assignments = []
    for i, model in enumerate(models):
        gpu_id = available_gpus[i % len(available_gpus)]["id"]
        device = f"cuda:{gpu_id}"
        assignments.append((model, device))
    
    return assignments

def run_single_evaluation(model_and_device, test_set_path, num_cases):
    """è¿è¡Œå•ä¸ªæ¨¡å‹çš„è¯„ä¼°"""
    model, device = model_and_device
    model_name = model['name']
    print(f"ï¿½ å¼€å§‹è¯„ä¼°: {model_name} (è®¾å¤‡: {device})")
    
    # æ„å»ºå‘½ä»¤
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # æ·»åŠ æ¯«ç§’é¿å…é‡å
    output_file = f"evaluation_{model['id']}_{device.replace(':', '_')}_{timestamp}.json"
    
    cmd = [
        "python", "multi_optimal_evaluator.py",
        "--model-path", model["path"],
        "--test-set", test_set_path,
        "--num-cases", str(num_cases),
        "--device", device,
        "--output", output_file
    ]
    
    # å¦‚æœæ˜¯LoRAæ¨¡å‹ï¼Œæ·»åŠ åŸºç¡€æ¨¡å‹è·¯å¾„
    if model["type"] == "lora":
        cmd.extend(["--base-model-path", "/mnt/cvda/cvda_avatar/1/textarena-selfplay-qwen/qwen"])
    
    start_time = time.time()
    
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿ä½¿ç”¨æŒ‡å®šGPU
        env = os.environ.copy()
        if device.startswith("cuda:"):
            gpu_id = device.split(":")[1]
            env["CUDA_VISIBLE_DEVICES"] = gpu_id
        
        print(f"ğŸ“‹ {model_name}: å¼€å§‹å¤„ç† {num_cases} ä¸ªæµ‹è¯•æ¡ˆä¾‹...")
        
        # è¿è¡Œè¯„ä¼°
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            cwd="/mnt/cvda/cvda_avatar/1/textarena-selfplay-qwen/evaluation",
            env=env
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        if result.returncode == 0:
            # å°è¯•ä»è¾“å‡ºæ–‡ä»¶è¯»å–ç»“æœ
            output_path = f"/mnt/cvda/cvda_avatar/1/textarena-selfplay-qwen/evaluation/{output_file}"
            if os.path.exists(output_path):
                with open(output_path, 'r', encoding='utf-8') as f:
                    eval_data = json.load(f)
                accuracy = eval_data["evaluation_info"]["accuracy"]
            else:
                # ä»stdoutè§£æå‡†ç¡®ç‡
                stdout_lines = result.stdout.split('\n')
                accuracy = None
                for line in stdout_lines:
                    if "å‡†ç¡®ç‡:" in line or "æœ€ç»ˆç»“æœ:" in line:
                        try:
                            # åŒ¹é…ç™¾åˆ†æ¯”
                            import re
                            match = re.search(r'(\d+\.?\d*)%', line)
                            if match:
                                accuracy = float(match.group(1))
                                break
                        except:
                            pass
                if accuracy is None:
                    accuracy = 0.0
            
            cases_per_sec = num_cases / duration if duration > 0 else 0
            print(f"âœ… {model_name} è¯„ä¼°å®Œæˆ!")
            print(f"   ğŸ“Š å‡†ç¡®ç‡: {accuracy:.1f}%")
            print(f"   â±ï¸  è€—æ—¶: {duration:.1f}ç§’ ({cases_per_sec:.1f} æ¡ˆä¾‹/ç§’)")
            print(f"   ğŸ’¾ ç»“æœæ–‡ä»¶: {output_file}")
            
            return {
                "model_id": model["id"],
                "model_name": model["name"],
                "model_path": model["path"],
                "device": device,
                "success": True,
                "accuracy": accuracy,
                "duration": duration,
                "cases_per_second": cases_per_sec,
                "output_file": output_file,
                "stdout": result.stdout,
                "stderr": result.stderr
            }
        else:
            error_msg = result.stderr[:200] + "..." if len(result.stderr) > 200 else result.stderr
            print(f"âŒ {model_name} è¯„ä¼°å¤±è´¥ ({device})")
            print(f"   ğŸ” é”™è¯¯: {error_msg}")
            return {
                "model_id": model["id"], 
                "model_name": model["name"],
                "model_path": model["path"],
                "device": device,
                "success": False,
                "accuracy": 0.0,
                "duration": duration,
                "cases_per_second": 0.0,
                "error": result.stderr,
                "stdout": result.stdout,
                "stderr": result.stderr
            }
            
    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        print(f"âŒ {model_name} è¯„ä¼°å¼‚å¸¸ ({device}): {str(e)}")
        return {
            "model_id": model["id"],
            "model_name": model["name"], 
            "model_path": model["path"],
            "device": device,
            "success": False,
            "accuracy": 0.0,
            "duration": duration,
            "cases_per_second": 0.0,
            "error": str(e)
        }

def main():
    parser = argparse.ArgumentParser(description="å®Œæ•´æ¨¡å‹è¯„ä¼°")
    parser.add_argument("--num-cases", type=int, default=100, help="æµ‹è¯•æ¡ˆä¾‹æ•°é‡")
    parser.add_argument("--test-set", type=str, 
                       default="/mnt/cvda/cvda_avatar/1/textarena-selfplay-qwen/data/processed/tictactoe_test_set_100_multi_optimal.json",
                       help="æµ‹è¯•é›†è·¯å¾„")
    parser.add_argument("--parallel", action="store_true", help="å¹¶è¡Œæ‰§è¡Œï¼ˆå¤šGPUæ¨¡å¼ï¼‰")
    parser.add_argument("--max-workers", type=int, default=None, help="æœ€å¤§å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--force-cpu", action="store_true", help="å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")
    
    args = parser.parse_args()
    
    # æ£€æµ‹å¯ç”¨GPU
    if args.force_cpu:
        available_gpus = []
        print("ï¿½ å¼ºåˆ¶CPUæ¨¡å¼")
    else:
        available_gpus = get_available_gpus()
    
    print("ï¿½ğŸš€ å®Œæ•´æ¨¡å‹è¯„ä¼°å¼€å§‹")
    print("=" * 60)
    print(f"ğŸ“‹ æµ‹è¯•é›†: {args.test_set}")
    print(f"ğŸ”¢ æµ‹è¯•æ¡ˆä¾‹æ•°é‡: {args.num_cases}")
    print(f"ğŸ¤– æ¨¡å‹æ•°é‡: {len(MODELS)}")
    print(f"ğŸ“Š æ€»è¯„ä¼°ä»»åŠ¡: {args.num_cases * len(MODELS)}")
    print(f"ğŸ”„ å¹¶è¡Œæ¨¡å¼: {'å¯ç”¨' if args.parallel else 'ç¦ç”¨'}")
    
    if available_gpus:
        print(f"ğŸ® æ£€æµ‹åˆ° {len(available_gpus)} ä¸ªGPU:")
        for gpu in available_gpus:
            print(f"  - GPU {gpu['id']}: {gpu['name']} ({gpu['memory']}GB)")
    else:
        print("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼")
    
    # åˆ†é…æ¨¡å‹åˆ°è®¾å¤‡
    model_assignments = assign_models_to_gpus(MODELS, available_gpus)
    
    print(f"\nğŸ“ æ¨¡å‹-è®¾å¤‡åˆ†é…:")
    for model, device in model_assignments:
        print(f"  - {model['name']}: {device}")
    print()
    
    start_time = time.time()
    results = []
    
    if args.parallel and len(available_gpus) > 0:
        # å¤šGPUå¹¶è¡Œæ‰§è¡Œ
        max_workers = args.max_workers or min(len(available_gpus), len(MODELS))
        print(f"âš¡ å¤šGPUå¹¶è¡Œæ‰§è¡Œæ¨¡å¼ (æœ€å¤§å·¥ä½œè¿›ç¨‹: {max_workers})")
        
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            future_to_model = {
                executor.submit(run_single_evaluation, model_assignment, args.test_set, args.num_cases): model_assignment[0] 
                for model_assignment in model_assignments
            }
            
            for future in as_completed(future_to_model):
                result = future.result()
                results.append(result)
    else:
        # ä¸²è¡Œæ‰§è¡Œï¼ˆæ¨èç”¨äºè°ƒè¯•ï¼‰
        print("ğŸ”„ ä¸²è¡Œæ‰§è¡Œæ¨¡å¼")
        for i, model_assignment in enumerate(model_assignments, 1):
            model, device = model_assignment
            print(f"\nğŸ“ è¿›åº¦: {i}/{len(MODELS)} - {model['name']} ({device})")
            result = run_single_evaluation(model_assignment, args.test_set, args.num_cases)
            results.append(result)
    
    end_time = time.time()
    total_duration = end_time - start_time
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    successful_results = [r for r in results if r["success"]]
    failed_results = [r for r in results if not r["success"]]
    
    # æŒ‰å‡†ç¡®ç‡æ’åº
    if successful_results:
        successful_results.sort(key=lambda x: x["accuracy"], reverse=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    summary_report = {
        "evaluation_info": {
            "timestamp": datetime.now().isoformat(),
            "test_set": args.test_set,
            "num_cases": args.num_cases,
            "total_models": len(MODELS),
            "successful_evaluations": len(successful_results),
            "failed_evaluations": len(failed_results),
            "total_duration": total_duration,
            "parallel_mode": args.parallel,
            "available_gpus": len(available_gpus),
            "gpu_info": available_gpus,
            "total_cases_evaluated": args.num_cases * len(successful_results),
            "overall_speed": (args.num_cases * len(successful_results)) / total_duration if total_duration > 0 else 0
        },
        "device_assignments": [
            {
                "model_id": model["id"],
                "model_name": model["name"], 
                "device": device
            } for model, device in model_assignments
        ],
        "results": results,
        "summary": {
            "best_model": successful_results[0] if successful_results else None,
            "worst_model": successful_results[-1] if successful_results else None,
            "average_accuracy": sum(r["accuracy"] for r in successful_results) / len(successful_results) if successful_results else 0
        }
    }
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    summary_file = f"full_evaluation_summary_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_report, f, ensure_ascii=False, indent=2)
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ‰ å®Œæ•´è¯„ä¼°å®Œæˆï¼")
    print("=" * 60)
    print(f"â±ï¸  æ€»è€—æ—¶: {total_duration/60:.1f} åˆ†é’Ÿ")
    print(f"ğŸš€ æ•´ä½“é€Ÿåº¦: {summary_report['evaluation_info']['overall_speed']:.1f} æ¡ˆä¾‹/ç§’")
    print(f"âœ… æˆåŠŸ: {len(successful_results)}/{len(MODELS)} æ¨¡å‹")
    if failed_results:
        print(f"âŒ å¤±è´¥: {len(failed_results)} æ¨¡å‹")
    print()
    
    if successful_results:
        print("ğŸ† æ¨¡å‹æ€§èƒ½æ’å:")
        for i, result in enumerate(successful_results, 1):
            duration = result["duration"]
            device = result.get("device", "unknown")
            cases_per_sec = args.num_cases / duration if duration > 0 else 0
            print(f"  {i}. {result['model_name']}: {result['accuracy']:.1f}% ({duration:.1f}ç§’, {cases_per_sec:.1f}æ¡ˆä¾‹/ç§’, {device})")
        
        print()
        print(f"ğŸ¥‡ æœ€ä½³æ¨¡å‹: {successful_results[0]['model_name']} ({successful_results[0]['accuracy']:.1f}%)")
        if len(successful_results) > 1:
            print(f"ğŸ“‰ æœ€å·®æ¨¡å‹: {successful_results[-1]['model_name']} ({successful_results[-1]['accuracy']:.1f}%)")
        print(f"ğŸ“Š å¹³å‡å‡†ç¡®ç‡: {summary_report['summary']['average_accuracy']:.1f}%")
    
    if failed_results:
        print("\nâŒ å¤±è´¥çš„æ¨¡å‹:")
        for result in failed_results:
            device = result.get("device", "unknown")
            error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')
            print(f"  - {result['model_name']} ({device}): {error_msg[:100]}...")
    
    print(f"\nğŸ’¾ æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {summary_file}")
    
    return summary_report

if __name__ == "__main__":
    main()
