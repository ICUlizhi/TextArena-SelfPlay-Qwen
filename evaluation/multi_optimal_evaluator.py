#!/usr/bin/env python3
"""
å¤šæœ€ä¼˜è§£æ¨¡å‹è¯„ä¼°å™¨
ä¸“é—¨ä¸ºtictactoe_test_set_100_multi_optimal.jsonè®¾è®¡çš„ç®€æ´è¯„ä¼°å·¥å…·
"""

import json
import re
import os
import sys
import argparse
from typing import Optional, List, Dict

try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("è­¦å‘Š: PyTorch/Transformersä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

class MultiOptimalEvaluator:
    """å¤šæœ€ä¼˜è§£è¯„ä¼°å™¨"""
    
    def __init__(self, model_path: str, base_model_path: str = None, device: str = "auto"):
        self.model_path = model_path
        self.base_model_path = base_model_path
        self.device = device
        self.model = None
        self.tokenizer = None
        self.model_name = os.path.basename(model_path)  # æ·»åŠ æ¨¡å‹åç§°
        self.load_model()
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorchä¸å¯ç”¨ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
        
        # è®¾å¤‡é€‰æ‹©
        if self.device == "auto":
            if torch.cuda.is_available():
                self.device = "cuda"
            else:
                self.device = "cpu"
        
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹åˆ°è®¾å¤‡: {self.device}")
        
        # æ£€æŸ¥è®¾å¤‡æ˜¯å¦å¯ç”¨
        if self.device.startswith("cuda"):
            device_id = int(self.device.split(":")[1]) if ":" in self.device else 0
            if device_id >= torch.cuda.device_count():
                raise RuntimeError(f"GPU {device_id} ä¸å­˜åœ¨ï¼Œç³»ç»Ÿå…±æœ‰ {torch.cuda.device_count()} ä¸ªGPU")
            
            # æ£€æŸ¥GPUå†…å­˜
            torch.cuda.set_device(device_id)
            memory_info = torch.cuda.get_device_properties(device_id)
            free_memory = torch.cuda.get_device_properties(device_id).total_memory
            print(f"ğŸ“Š GPU {device_id} å†…å­˜: {free_memory // (1024**3)}GB ({memory_info.name})")
        
        # åˆ¤æ–­æ˜¯å¾®è°ƒæ¨¡å‹è¿˜æ˜¯åŸºç¡€æ¨¡å‹
        if os.path.exists(os.path.join(self.model_path, "adapter_config.json")):
            # å¾®è°ƒæ¨¡å‹ (LoRA)
            print("ğŸ“¦ åŠ è½½å¾®è°ƒæ¨¡å‹...")
            if not self.base_model_path:
                self.base_model_path = "/mnt/cvda/cvda_avatar/1/textarena-selfplay-qwen/qwen"
            
            print(f"ğŸ”— åŸºç¡€æ¨¡å‹è·¯å¾„: {self.base_model_path}")
            print(f"ğŸ”— LoRAé€‚é…å™¨è·¯å¾„: {self.model_path}")
            
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.base_model_path):
                raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.base_model_path}")
            if not os.path.exists(self.model_path):
                raise FileNotFoundError(f"LoRAæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            print("ğŸ”„ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_path, trust_remote_code=True)
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_path,
                device_map=self.device,
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            # åŠ è½½LoRAé€‚é…å™¨
            print("ğŸ”„ æ­£åœ¨åŠ è½½LoRAé€‚é…å™¨...")
            self.model = PeftModel.from_pretrained(base_model, self.model_path)
            print("âœ… å¾®è°ƒæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        else:
            # åŸºç¡€æ¨¡å‹
            print("ğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹...")
            print(f"ğŸ”— æ¨¡å‹è·¯å¾„: {self.model_path}")
            
            if not os.path.exists(self.model_path):
                raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                device_map=self.device,
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    def generate_response(self, prompt: str) -> str:
        """ç”Ÿæˆæ¨¡å‹å“åº”"""
        if not TORCH_AVAILABLE:
            raise RuntimeError("PyTorchä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ¨¡å‹æ¨ç†")
        
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œæ¨ç†")
        
        try:
            # å®é™…æ¨¡å‹æ¨ç†
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            # ç§»é™¤åŸå§‹promptéƒ¨åˆ†
            response = response[len(prompt):].strip()
            return response
            
        except Exception as e:
            raise RuntimeError(f"ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {e}")
    
    def extract_move(self, response: str) -> Optional[str]:
        """ä»å“åº”ä¸­æå–ç§»åŠ¨"""
        if not response:
            return None
        
        # å¤šç§æ¨¡å¼åŒ¹é…
        patterns = [
            r'ç­”æ¡ˆ:\s*\[(\d)\]',           # ç­”æ¡ˆ: [æ•°å­—]
            r'é€‰æ‹©:\s*\[(\d)\]',           # é€‰æ‹©: [æ•°å­—]
            r'æœ€ç»ˆé€‰æ‹©:\s*\[(\d)\]',       # æœ€ç»ˆé€‰æ‹©: [æ•°å­—]
            r'æˆ‘é€‰æ‹©:\s*\[(\d)\]',         # æˆ‘é€‰æ‹©: [æ•°å­—]
            r'\[(\d)\]',                   # ä»»ä½• [æ•°å­—]
            r'ä½ç½®\s*(\d)',                # ä½ç½®æ•°å­—
            r'é€‰æ‹©ä½ç½®\s*(\d)',            # é€‰æ‹©ä½ç½®æ•°å­—
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, response)
            if matches:
                move_num = matches[-1]  # å–æœ€åä¸€ä¸ªåŒ¹é…
                return f"[{move_num}]"
        return None
    
    def is_move_optimal(self, predicted_move: str, case: Dict) -> bool:
        """æ£€æŸ¥é¢„æµ‹çš„ç§»åŠ¨æ˜¯å¦ä¸ºæœ€ä¼˜è§£"""
        if predicted_move is None:
            return False
        
        # å¦‚æœæµ‹è¯•ç”¨ä¾‹æœ‰å¤šä¸ªæœ€ä¼˜è§£åˆ—è¡¨ï¼Œæ£€æŸ¥é¢„æµ‹æ˜¯å¦åœ¨å…¶ä¸­
        if 'optimal_moves' in case:
            return predicted_move in case['optimal_moves']
        
        # å‘åå…¼å®¹ï¼šå¦‚æœåªæœ‰å•ä¸ªæœ€ä¼˜è§£ï¼Œä½¿ç”¨ä¼ ç»Ÿæ¯”è¾ƒ
        optimal_move = case.get('optimal_move')
        return predicted_move == optimal_move
    
    def create_prompt(self, case: Dict) -> str:
        """åˆ›å»ºæç¤ºè¯"""
        return f"""ä½ æ˜¯ä¸€ä¸ªäº•å­—æ£‹ä¸“å®¶ï¼Œéœ€è¦åœ¨å½“å‰å±€é¢ä¸‹é€‰æ‹©æœ€ä¼˜çš„è½å­ä½ç½®ã€‚

ã€äº•å­—æ£‹æ¸¸æˆè§„åˆ™ã€‘
1. ä¸¤åç©å®¶è½®æµåœ¨3Ã—3çš„æ£‹ç›˜ä¸Šæ”¾ç½®è‡ªå·±çš„æ ‡è®°ï¼ˆXæˆ–Oï¼‰
2. è·èƒœæ¡ä»¶ï¼šç‡å…ˆåœ¨æ¨ªè¡Œã€ç«–åˆ—æˆ–å¯¹è§’çº¿ä¸Šè¿æˆ3ä¸ªè‡ªå·±çš„æ ‡è®°
3. å¹³å±€æ¡ä»¶ï¼šæ£‹ç›˜å¡«æ»¡ä¸”æ— äººè·èƒœ
4. æˆ˜ç•¥è¦ç‚¹ï¼š
   - ä¼˜å…ˆçº§1ï¼šå¦‚æœä½ èƒ½è·èƒœï¼Œç«‹å³è·èƒœ
   - ä¼˜å…ˆçº§2ï¼šå¦‚æœå¯¹æ‰‹ä¸‹ä¸€æ­¥èƒ½è·èƒœï¼Œå¿…é¡»é˜»æ­¢
   - ä¼˜å…ˆçº§3ï¼šåˆ›é€ å¤šé‡å¨èƒï¼ˆforkï¼‰

ã€å½“å‰æ£‹ç›˜çŠ¶æ€ã€‘
{case['board_state']}

ã€ä½ç½®ç¼–å·å¯¹åº”å…³ç³»ã€‘
0 | 1 | 2
---------
3 | 4 | 5
---------
6 | 7 | 8

ã€æ¸¸æˆä¿¡æ¯ã€‘
- ä½ æ˜¯ç©å®¶{case['player']}ï¼Œç°åœ¨è½®åˆ°ä½ è½å­
- å¯é€‰çš„ä½ç½®æœ‰ï¼š{', '.join(case['available_moves'])}

ã€è·èƒœçº¿è·¯è¯´æ˜ã€‘
- æ¨ªè¡Œï¼š0-1-2, 3-4-5, 6-7-8
- ç«–åˆ—ï¼š0-3-6, 1-4-7, 2-5-8  
- å¯¹è§’çº¿ï¼š0-4-8, 2-4-6

è¯·è¿›è¡Œè¯¦ç»†çš„æ€è€ƒåˆ†æï¼Œç„¶åç»™å‡ºä½ çš„æœ€ç»ˆé€‰æ‹©ã€‚

ã€æ€è€ƒæ­¥éª¤ã€‘
1. ã€å±€é¢åˆ†æã€‘åˆ†æå½“å‰æ£‹ç›˜çŠ¶æ€ï¼Œè¯†åˆ«å·²æœ‰çš„æ£‹å­åˆ†å¸ƒ
2. ã€å¨èƒæ£€æµ‹ã€‘æ£€æŸ¥æ˜¯å¦æœ‰ç«‹å³è·èƒœæœºä¼šæˆ–éœ€è¦é˜»æ­¢å¯¹æ‰‹è·èƒœ
3. ã€ç­–ç•¥å¯¼å‘ã€‘ç¡®å®šå½“å‰åº”è¯¥é‡‡ç”¨çš„ç­–ç•¥ï¼ˆè¿›æ”»/é˜²å®ˆ/å¹³è¡¡ï¼‰
4. ã€å€™é€‰è¯„ä¼°ã€‘è¯„ä¼°å„ä¸ªå¯é€‰ä½ç½®çš„ä»·å€¼å’Œé£é™©
5. ã€æœ€ç»ˆå†³ç­–ã€‘é€‰æ‹©æœ€ä¼˜ä½ç½®å¹¶è¯´æ˜ç†ç”±

æœ€åè¯·ç”¨æ ¼å¼"ç­”æ¡ˆ: [æ•°å­—]"ç»™å‡ºä½ çš„é€‰æ‹©ã€‚
"""
    
    def evaluate(self, test_set_path: str, num_cases: Optional[int] = None) -> tuple[float, List[Dict]]:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        # åŠ è½½æµ‹è¯•é›†
        with open(test_set_path, 'r', encoding='utf-8') as f:
            test_cases = json.load(f)
        
        if num_cases:
            test_cases = test_cases[:num_cases]
        
        print(f"ğŸ“‹ åŠ è½½äº† {len(test_cases)} ä¸ªæµ‹è¯•æ¡ˆä¾‹")
        print(f"ğŸ¤– å½“å‰æ¨¡å‹: {self.model_name}")
        print(f"ğŸ“ è¿è¡Œè®¾å¤‡: {self.device}")
        print("-" * 80)
        
        correct_count = 0
        total_count = len(test_cases)
        detailed_results = []
        
        for i, case in enumerate(test_cases, 1):
            print(f"\nğŸ”„ æ¡ˆä¾‹ {i}/{total_count} (ID: {case.get('id', i)})")
            print(f"   æ£‹ç›˜: {case.get('stage', 'unknown')} | éš¾åº¦: {case.get('difficulty', 'unknown')} | ç©å®¶: {case.get('player', 'unknown')}")
            print(f"   æœ€ä¼˜è§£: {case.get('optimal_moves', [])}")
            
            # æ˜¾ç¤ºå½“å‰æ£‹ç›˜çŠ¶æ€
            board_state = case.get('board_state', '')
            if board_state:
                print(f"   ğŸ“‹ å½“å‰æ£‹ç›˜:")
                # å°†æ£‹ç›˜çŠ¶æ€æŒ‰è¡Œåˆ†å‰²å¹¶ç¼©è¿›æ˜¾ç¤º
                board_lines = board_state.split('\n')
                for line in board_lines:
                    print(f"      {line}")
            
            # ç”Ÿæˆæç¤ºè¯
            prompt = self.create_prompt(case)
            
            # è·å–æ¨¡å‹å“åº”
            print("   ğŸ”® æ¨¡å‹æ€è€ƒä¸­...")
            response = self.generate_response(prompt)
            
            # æå–é¢„æµ‹ç§»åŠ¨
            predicted_move = self.extract_move(response)
            
            # æ£€æŸ¥æ˜¯å¦æ­£ç¡®
            is_correct = self.is_move_optimal(predicted_move, case)
            
            if is_correct:
                correct_count += 1
            
            # å®æ—¶æ˜¾ç¤ºç»“æœ
            status = "âœ… æ­£ç¡®" if is_correct else "âŒ é”™è¯¯"
            current_accuracy = (correct_count / i) * 100
            print(f"   ğŸ“¤ æ¨¡å‹é€‰æ‹©: {predicted_move}")
            print(f"   ğŸ“Š ç»“æœ: {status} | å½“å‰å‡†ç¡®ç‡: {current_accuracy:.1f}% ({correct_count}/{i})")
            
            # æ˜¾ç¤ºæ¨¡å‹è¾“å‡ºçš„å…³é”®éƒ¨åˆ†
            if len(response) > 150:
                output_preview = response[:150] + "..."
            else:
                output_preview = response
            print(f"   ğŸ’­ æ¨¡å‹å›ç­”: {output_preview}")
            
            # è®°å½•è¯¦ç»†ç»“æœ
            result_item = {
                "case_id": case.get("id", i),
                "board_state": case.get("board_state", ""),
                "player": case.get("player", ""),
                "available_moves": case.get("available_moves", []),
                "optimal_moves": case.get("optimal_moves", [case.get("optimal_move", "")]),
                "model_output": response,
                "predicted_move": predicted_move,
                "is_correct": is_correct,
                "difficulty": case.get("difficulty", ""),
                "stage": case.get("stage", ""),
                "move_type": case.get("move_type", "")
            }
            detailed_results.append(result_item)
            
            # æ¯10ä¸ªæ¡ˆä¾‹æ˜¾ç¤ºä¸€æ¬¡ç»Ÿè®¡
            if i % 10 == 0 or i == total_count:
                print(f"\nğŸ“ˆ é˜¶æ®µæ€§ç»Ÿè®¡ ({i}/{total_count}):")
                print(f"   æ­£ç¡®: {correct_count} | é”™è¯¯: {i - correct_count} | å‡†ç¡®ç‡: {current_accuracy:.1f}%")
                if i < total_count:
                    print("-" * 40)
        
        accuracy = (correct_count / total_count) * 100
        print(f"\nğŸ¯ æœ€ç»ˆç»“æœ: {accuracy:.2f}% ({correct_count}/{total_count})")
        return accuracy, detailed_results

def load_test_cases(test_set_path: str, num_cases: Optional[int] = None) -> List[Dict]:
    """åŠ è½½æµ‹è¯•æ¡ˆä¾‹"""
    if not os.path.exists(test_set_path):
        print(f"âŒ æµ‹è¯•é›†æ–‡ä»¶ä¸å­˜åœ¨: {test_set_path}")
        return []
    
    with open(test_set_path, 'r', encoding='utf-8') as f:
        test_cases = json.load(f)
    
    if num_cases:
        test_cases = test_cases[:num_cases]
    
    print(f"âœ… åŠ è½½äº† {len(test_cases)} ä¸ªæµ‹è¯•æ¡ˆä¾‹")
    return test_cases

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¤šæœ€ä¼˜è§£æ¨¡å‹è¯„ä¼°å™¨")
    parser.add_argument("--model-path", type=str, required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--base-model-path", type=str, 
                       default="/mnt/cvda/cvda_avatar/1/textarena-selfplay-qwen/qwen",
                       help="åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆå¾®è°ƒæ¨¡å‹éœ€è¦ï¼‰")
    parser.add_argument("--test-set", type=str, required=True, help="æµ‹è¯•é›†è·¯å¾„")
    parser.add_argument("--num-cases", type=int, help="æµ‹è¯•æ¡ˆä¾‹æ•°é‡ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--device", type=str, default="auto", help="è®¾å¤‡ï¼ˆcuda:0, cpuç­‰ï¼‰")
    parser.add_argument("--output", type=str, help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    
    args = parser.parse_args()
    
    print("ğŸ¯ å¤šæœ€ä¼˜è§£æ¨¡å‹è¯„ä¼°å™¨")
    print("=" * 50)
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"æµ‹è¯•é›†: {args.test_set}")
    print(f"è®¾å¤‡: {args.device}")
    if args.num_cases:
        print(f"æµ‹è¯•æ¡ˆä¾‹æ•°é‡: {args.num_cases}")
    print()
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = MultiOptimalEvaluator(
        model_path=args.model_path,
        base_model_path=args.base_model_path,
        device=args.device
    )
    
    # æ‰§è¡Œè¯„ä¼°
    accuracy, detailed_results = evaluator.evaluate(args.test_set, args.num_cases)
    
    # å‡†å¤‡å®Œæ•´çš„è¯„ä¼°ç»“æœ
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    evaluation_summary = {
        "evaluation_info": {
            "timestamp": datetime.now().isoformat(),
            "model_path": args.model_path,
            "test_set": args.test_set,
            "device": args.device,
            "num_cases": args.num_cases or len(detailed_results),
            "accuracy": accuracy
        },
        "summary": {
            "total_cases": len(detailed_results),
            "correct_cases": sum(1 for r in detailed_results if r["is_correct"]),
            "accuracy_percentage": accuracy,
            "difficulty_breakdown": {},
            "stage_breakdown": {},
            "move_type_breakdown": {}
        },
        "detailed_results": detailed_results
    }
    
    # ç»Ÿè®¡åˆ†æ
    for result in detailed_results:
        # éš¾åº¦ç»Ÿè®¡
        difficulty = result["difficulty"]
        if difficulty not in evaluation_summary["summary"]["difficulty_breakdown"]:
            evaluation_summary["summary"]["difficulty_breakdown"][difficulty] = {"total": 0, "correct": 0}
        evaluation_summary["summary"]["difficulty_breakdown"][difficulty]["total"] += 1
        if result["is_correct"]:
            evaluation_summary["summary"]["difficulty_breakdown"][difficulty]["correct"] += 1
        
        # é˜¶æ®µç»Ÿè®¡
        stage = result["stage"]
        if stage not in evaluation_summary["summary"]["stage_breakdown"]:
            evaluation_summary["summary"]["stage_breakdown"][stage] = {"total": 0, "correct": 0}
        evaluation_summary["summary"]["stage_breakdown"][stage]["total"] += 1
        if result["is_correct"]:
            evaluation_summary["summary"]["stage_breakdown"][stage]["correct"] += 1
        
        # ç§»åŠ¨ç±»å‹ç»Ÿè®¡
        move_type = result["move_type"]
        if move_type not in evaluation_summary["summary"]["move_type_breakdown"]:
            evaluation_summary["summary"]["move_type_breakdown"][move_type] = {"total": 0, "correct": 0}
        evaluation_summary["summary"]["move_type_breakdown"][move_type]["total"] += 1
        if result["is_correct"]:
            evaluation_summary["summary"]["move_type_breakdown"][move_type]["correct"] += 1
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    for category in ["difficulty_breakdown", "stage_breakdown", "move_type_breakdown"]:
        for key in evaluation_summary["summary"][category]:
            stats = evaluation_summary["summary"][category][key]
            stats["accuracy"] = (stats["correct"] / stats["total"] * 100) if stats["total"] > 0 else 0
    
    print("=" * 50)
    print(f"ğŸ‰ è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“Š å‡†ç¡®ç‡: {accuracy:.2f}%")
    print()
    
    # è¾“å‡ºç®€è¦ç»Ÿè®¡
    print("ğŸ“ˆ è¯¦ç»†ç»Ÿè®¡:")
    print(f"æ€»æ¡ˆä¾‹æ•°: {evaluation_summary['summary']['total_cases']}")
    print(f"æ­£ç¡®æ¡ˆä¾‹æ•°: {evaluation_summary['summary']['correct_cases']}")
    print()
    
    if evaluation_summary["summary"]["difficulty_breakdown"]:
        print("æŒ‰éš¾åº¦åˆ†ç±»:")
        for difficulty, stats in evaluation_summary["summary"]["difficulty_breakdown"].items():
            print(f"  {difficulty}: {stats['correct']}/{stats['total']} ({stats['accuracy']:.1f}%)")
    
    if evaluation_summary["summary"]["stage_breakdown"]:
        print("æŒ‰é˜¶æ®µåˆ†ç±»:")
        for stage, stats in evaluation_summary["summary"]["stage_breakdown"].items():
            print(f"  {stage}: {stats['correct']}/{stats['total']} ({stats['accuracy']:.1f}%)")
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSON
    if args.output:
        output_file = args.output
    else:
        model_name = os.path.basename(args.model_path)
        output_file = f"multi_optimal_evaluation_{model_name}_{timestamp}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation_summary, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªæ¡ˆä¾‹çš„è¯¦ç»†ä¿¡æ¯
    print("\nğŸ” å‰3ä¸ªæ¡ˆä¾‹çš„è¯¦ç»†ä¿¡æ¯:")
    for i, result in enumerate(detailed_results[:3], 1):
        print(f"\næ¡ˆä¾‹ {i} (ID: {result['case_id']}):")
        print(f"  æ£‹ç›˜çŠ¶æ€:\n{result['board_state']}")
        print(f"  ç©å®¶: {result['player']}")
        print(f"  å¯é€‰ä½ç½®: {result['available_moves']}")
        print(f"  æœ€ä¼˜è§£: {result['optimal_moves']}")
        print(f"  æ¨¡å‹é€‰æ‹©: {result['predicted_move']}")
        print(f"  æ˜¯å¦æ­£ç¡®: {'âœ…' if result['is_correct'] else 'âŒ'}")
        print(f"  æ¨¡å‹è¾“å‡º: {result['model_output'][:100]}..." if len(result['model_output']) > 100 else f"  æ¨¡å‹è¾“å‡º: {result['model_output']}")
    
    return evaluation_summary

if __name__ == "__main__":
    main()
