#!/bin/bash

# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•æ‰§è¡Œ
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
PROJECT_ROOT="$( cd "$SCRIPT_DIR/../.." &> /dev/null && pwd )"
cd "$PROJECT_ROOT"

echo "=================================================================================="
echo "å¤šGPUå¹¶è¡Œè®­ç»ƒ6ç§CoTæ¨¡å‹"
echo "å½“å‰å·¥ä½œç›®å½•: $(pwd)"
echo "=================================================================================="

# æ£€æŸ¥llamafactory-cliæ˜¯å¦å¯ç”¨
if ! command -v llamafactory-cli &> /dev/null; then
    echo "âŒ llamafactory-cli æœªæ‰¾åˆ°ï¼Œå°è¯•ä½¿ç”¨condaæ¿€æ´»ç¯å¢ƒ..."
    source ~/anaconda3/etc/profile.d/conda.sh
    conda activate tictactoe
    if ! command -v llamafactory-cli &> /dev/null; then
        echo "âŒ llamafactory-cli ä»æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿LLaMA Factoryå·²æ­£ç¡®å®‰è£…"
        exit 1
    fi
fi

echo "âœ… llamafactory-cli å¯ç”¨: $(which llamafactory-cli)"

# æ£€æŸ¥å¯ç”¨GPU
nvidia-smi --list-gpus
echo ""

# æ£€æŸ¥è®­ç»ƒæ•°æ®æ˜¯å¦å­˜åœ¨
echo "ğŸ” æ£€æŸ¥è®­ç»ƒæ•°æ®æ–‡ä»¶..."
data_files_found=0
for cot_type in tiny short medium long very_long ultra_long; do
    pattern="data/processed/long_cot_sft_data_${cot_type}_*.json"
    files=($(ls $pattern 2>/dev/null))
    if [ ${#files[@]} -gt 0 ]; then
        latest_file="${files[-1]}"  # è·å–æœ€æ–°æ–‡ä»¶
        echo "âœ… æ‰¾åˆ° ${cot_type} æ•°æ®: $latest_file"
        data_files_found=$((data_files_found + 1))
    else
        echo "âŒ æœªæ‰¾åˆ° ${cot_type} æ•°æ®æ–‡ä»¶: $pattern"
    fi
done

if [ $data_files_found -eq 0 ]; then
    echo "âŒ æœªæ‰¾åˆ°ä»»ä½•è®­ç»ƒæ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®ç”Ÿæˆè„šæœ¬"
    exit 1
fi

echo "âœ… æ‰¾åˆ° $data_files_found ä¸ªè®­ç»ƒæ•°æ®æ–‡ä»¶"
echo ""

# å®šä¹‰CoTç±»å‹å’Œå¯¹åº”çš„GPU
declare -A COT_TYPES=(
    ["tiny"]="1"
    ["short"]="2" 
    ["medium"]="3"
    ["long"]="4"
    ["very_long"]="5"
    ["ultra_long"]="6"
)

# åˆ›å»ºå¿…è¦ç›®å½•
mkdir -p models
mkdir -p llama_factory_configs
mkdir -p logs

echo "ğŸš€ å¼€å§‹åœ¨6ä¸ªGPUä¸Šå¹¶è¡Œè®­ç»ƒ..."

# ä¸ºæ¯ä¸ªCoTç±»å‹åˆ›å»ºè®­ç»ƒé…ç½®å¹¶å¯åŠ¨è®­ç»ƒ
for cot_type in "${!COT_TYPES[@]}"; do
    gpu_id=${COT_TYPES[$cot_type]}
    
    # æŸ¥æ‰¾å¯¹åº”çš„æ•°æ®æ–‡ä»¶
    pattern="data/processed/long_cot_sft_data_${cot_type}_*.json"
    files=($(ls $pattern 2>/dev/null))
    if [ ${#files[@]} -eq 0 ]; then
        echo "âŒ è·³è¿‡ ${cot_type}ï¼šæœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ $pattern"
        continue
    fi
    
    # è·å–æœ€æ–°çš„æ•°æ®æ–‡ä»¶
    latest_file="${files[-1]}"
    full_path="$(realpath "$latest_file")"
    
    echo "ğŸ“ ä¸º ${cot_type} åˆ›å»ºé…ç½®ï¼Œä½¿ç”¨æ•°æ®æ–‡ä»¶: $latest_file"
    
    # è®¾ç½®cutoff_len
    cutoff_len=1024
    if [[ "$cot_type" == "very_long" || "$cot_type" == "ultra_long" ]]; then
        cutoff_len=2048
    fi
    
    # æ›´æ–°dataset_info.jsonä¸­çš„æ–‡ä»¶è·¯å¾„
    python3 -c "
import json
import os

dataset_info_path = 'llama_factory_configs/dataset_info.json'
if os.path.exists(dataset_info_path):
    with open(dataset_info_path, 'r') as f:
        dataset_info = json.load(f)
else:
    dataset_info = {}

dataset_info['tictactoe_${cot_type}_cot'] = {
    'file_name': '$full_path',
    'formatting': 'sharegpt',
    'columns': {
        'messages': 'conversations'
    },
    'tags': {
        'role_tag': 'from',
        'content_tag': 'value',
        'user_tag': 'human',
        'assistant_tag': 'gpt'
    }
}

with open(dataset_info_path, 'w') as f:
    json.dump(dataset_info, f, indent=2)
"
    
    # åˆ›å»ºè®­ç»ƒé…ç½®æ–‡ä»¶
    cat > "llama_factory_configs/qwen_${cot_type}_sft_config.yaml" << EOF
### model
model_name_or_path: ./qwen
model_revision: main

### method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.1

### dataset
dataset: tictactoe_${cot_type}_cot
dataset_dir: ./llama_factory_configs
template: qwen
cutoff_len: $cutoff_len
max_samples: 1000
overwrite_cache: true
preprocessing_num_workers: 4

### output
output_dir: ./models/qwen_${cot_type}_cot_lora
logging_steps: 10
save_steps: 100
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 5.0e-4
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

### eval
val_size: 0.1
per_device_eval_batch_size: 2
eval_strategy: steps
eval_steps: 100
EOF

    echo "ğŸ“ åˆ›å»º ${cot_type} é…ç½®æ–‡ä»¶å®Œæˆ"
    
    # åœ¨æŒ‡å®šGPUä¸Šå¯åŠ¨è®­ç»ƒï¼ˆåå°è¿è¡Œï¼‰
    echo "ğŸš€ åœ¨GPU ${gpu_id} ä¸Šå¯åŠ¨ ${cot_type} æ¨¡å‹è®­ç»ƒ..."
    
    # ä½¿ç”¨nohupåœ¨åå°è¿è¡Œï¼Œå¹¶é‡å®šå‘è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶
    nohup bash -c "
        source ~/anaconda3/etc/profile.d/conda.sh
        conda activate tictactoe
        export CUDA_VISIBLE_DEVICES=${gpu_id}
        cd '$PROJECT_ROOT'
        echo \"å¼€å§‹è®­ç»ƒ ${cot_type} æ¨¡å‹ (GPU ${gpu_id}) - \$(date)\" >> logs/train_${cot_type}.log
        echo \"å·¥ä½œç›®å½•: \$(pwd)\" >> logs/train_${cot_type}.log
        echo \"æ•°æ®æ–‡ä»¶: $latest_file\" >> logs/train_${cot_type}.log
        echo \"é…ç½®æ–‡ä»¶: llama_factory_configs/qwen_${cot_type}_sft_config.yaml\" >> logs/train_${cot_type}.log
        echo \"---\" >> logs/train_${cot_type}.log
        llamafactory-cli train llama_factory_configs/qwen_${cot_type}_sft_config.yaml >> logs/train_${cot_type}.log 2>&1
        exit_code=\$?
        echo \"${cot_type} æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œé€€å‡ºç : \$exit_code - \$(date)\" >> logs/train_${cot_type}.log
    " &
    
    # ä¿å­˜è¿›ç¨‹ID
    echo $! > "logs/train_${cot_type}.pid"
    
    echo "   âœ… ${cot_type} è®­ç»ƒå·²å¯åŠ¨ï¼ŒPID: $(cat logs/train_${cot_type}.pid)"
    echo "   ğŸ“„ æ—¥å¿—æ–‡ä»¶: logs/train_${cot_type}.log"
    echo ""
    
    # çŸ­æš‚å»¶è¿Ÿé¿å…åŒæ—¶å¯åŠ¨å¯¼è‡´èµ„æºå†²çª
    sleep 5
done

echo "=================================================================================="
echo "æ‰€æœ‰è®­ç»ƒä»»åŠ¡å·²å¯åŠ¨ï¼"
echo "=================================================================================="
echo ""
echo "ğŸ“Š è®­ç»ƒçŠ¶æ€ç›‘æ§ï¼š"
echo "   - æŸ¥çœ‹æ‰€æœ‰è®­ç»ƒè¿›ç¨‹: ps aux | grep llamafactory-cli"
echo "   - æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µ: nvidia-smi"
echo "   - æŸ¥çœ‹è®­ç»ƒæ—¥å¿—: tail -f logs/train_<cot_type>.log"
echo ""

echo "ğŸ” å½“å‰è®­ç»ƒè¿›ç¨‹ï¼š"
for cot_type in "${!COT_TYPES[@]}"; do
    gpu_id=${COT_TYPES[$cot_type]}
    if [ -f "logs/train_${cot_type}.pid" ]; then
        pid=$(cat "logs/train_${cot_type}.pid")
        if ps -p $pid > /dev/null; then
            echo "   âœ… ${cot_type} (GPU ${gpu_id}): è¿è¡Œä¸­ (PID: $pid)"
        else
            echo "   âŒ ${cot_type} (GPU ${gpu_id}): å·²åœæ­¢ (PID: $pid)"
        fi
    fi
done

echo ""
echo "ğŸ“ ç›‘æ§è„šæœ¬å·²åˆ›å»ºï¼š"
echo "   - è¿è¡Œ './monitor_training.sh' æ¥ç›‘æ§è®­ç»ƒè¿›åº¦"
echo "   - è¿è¡Œ './stop_all_training.sh' æ¥åœæ­¢æ‰€æœ‰è®­ç»ƒ"

# åˆ›å»ºç›‘æ§è„šæœ¬
cat > monitor_training.sh << 'EOF'
#!/bin/bash
echo "=================================================================================="
echo "è®­ç»ƒè¿›åº¦ç›‘æ§"
echo "=================================================================================="

declare -A COT_TYPES=(
    ["tiny"]="1"
    ["short"]="2" 
    ["medium"]="3"
    ["long"]="4"
    ["very_long"]="5"
    ["ultra_long"]="6"
)

while true; do
    clear
    echo "ğŸ” è®­ç»ƒçŠ¶æ€ $(date)"
    echo "--------------------------------------------------------------------------------"
    
    for cot_type in "${!COT_TYPES[@]}"; do
        gpu_id=${COT_TYPES[$cot_type]}
        if [ -f "logs/train_${cot_type}.pid" ]; then
            pid=$(cat "logs/train_${cot_type}.pid")
            if ps -p $pid > /dev/null; then
                echo "âœ… ${cot_type} (GPU ${gpu_id}): è¿è¡Œä¸­ (PID: $pid)"
                if [ -f "logs/train_${cot_type}.log" ]; then
                    last_line=$(tail -1 "logs/train_${cot_type}.log" 2>/dev/null)
                    echo "   æœ€æ–°: $last_line"
                fi
            else
                echo "âŒ ${cot_type} (GPU ${gpu_id}): å·²åœæ­¢ (PID: $pid)"
            fi
        else
            echo "âšª ${cot_type} (GPU ${gpu_id}): æœªå¯åŠ¨"
        fi
        echo ""
    done
    
    echo "--------------------------------------------------------------------------------"
    echo "ğŸ–¥ï¸  GPUä½¿ç”¨æƒ…å†µ:"
    nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits | while read line; do
        echo "   $line"
    done
    
    echo ""
    echo "æŒ‰ Ctrl+C é€€å‡ºç›‘æ§"
    sleep 10
done
EOF

# åˆ›å»ºåœæ­¢è„šæœ¬
cat > stop_all_training.sh << 'EOF'
#!/bin/bash
echo "åœæ­¢æ‰€æœ‰è®­ç»ƒè¿›ç¨‹..."

declare -A COT_TYPES=(
    ["tiny"]="1"
    ["short"]="2" 
    ["medium"]="3"
    ["long"]="4"
    ["very_long"]="5"
    ["ultra_long"]="6"
)

for cot_type in "${!COT_TYPES[@]}"; do
    if [ -f "logs/train_${cot_type}.pid" ]; then
        pid=$(cat "logs/train_${cot_type}.pid")
        if ps -p $pid > /dev/null; then
            echo "åœæ­¢ ${cot_type} è®­ç»ƒ (PID: $pid)..."
            kill $pid
        fi
        rm -f "logs/train_${cot_type}.pid"
    fi
done

echo "æ‰€æœ‰è®­ç»ƒè¿›ç¨‹å·²åœæ­¢ã€‚"
EOF

chmod +x monitor_training.sh
chmod +x stop_all_training.sh

echo "ğŸ“‹ ä½¿ç”¨è¯´æ˜ï¼š"
echo "   - ç›‘æ§è®­ç»ƒ: ./monitor_training.sh"
echo "   - åœæ­¢è®­ç»ƒ: ./stop_all_training.sh"
echo "   - æŸ¥çœ‹æ—¥å¿—: tail -f logs/train_<type>.log"
echo ""
echo "ğŸ¯ è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹å°†ä¿å­˜åœ¨ models/ ç›®å½•ä¸‹"
