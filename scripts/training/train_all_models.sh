#!/bin/bash

# ç®€åŒ–ç‰ˆæ‰¹é‡è®­ç»ƒè„šæœ¬
# è®­ç»ƒå…­ç§CoTé•¿åº¦çš„æ¨¡å‹

set -e

echo "=========================================="
echo "CoTæ¨¡å‹æ‰¹é‡è®­ç»ƒè„šæœ¬"
echo "=========================================="

# è®¾ç½®ç¯å¢ƒå˜é‡
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
export CUDA_VISIBLE_DEVICES=7

# æ£€æŸ¥åŸºç¡€æ¨¡å‹
BASE_MODEL="/mnt/cvda/cvda_avatar/1/textarena-selfplay-qwen/qwen"
if [ ! -d "$BASE_MODEL" ]; then
    echo "âŒ åŸºç¡€æ¨¡å‹ä¸å­˜åœ¨: $BASE_MODEL"
    exit 1
fi

# æ£€æŸ¥æ•°æ®æ–‡ä»¶
DATA_DIR="data/processed"
echo "ğŸ” æ£€æŸ¥è®­ç»ƒæ•°æ®æ–‡ä»¶..."

COT_TYPES=("tiny" "short" "medium" "long" "very_long" "ultra_long")
AVAILABLE_DATA=()

for cot_type in "${COT_TYPES[@]}"; do
    # æ‰¾åˆ°æœ€æ–°çš„æ•°æ®æ–‡ä»¶
    latest_file=$(ls -t ${DATA_DIR}/long_cot_sft_data_${cot_type}_*.json 2>/dev/null | head -n1)
    if [ -n "$latest_file" ]; then
        echo "âœ… $cot_type: $latest_file"
        AVAILABLE_DATA+=("$cot_type:$latest_file")
    else
        echo "âŒ $cot_type: æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶"
    fi
done

if [ ${#AVAILABLE_DATA[@]} -eq 0 ]; then
    echo "âŒ æœªæ‰¾åˆ°ä»»ä½•è®­ç»ƒæ•°æ®æ–‡ä»¶!"
    echo "è¯·å…ˆè¿è¡Œ: python generate_all_cot_lengths.py"
    exit 1
fi

echo "ğŸ“Š æ‰¾åˆ° ${#AVAILABLE_DATA[@]} ä¸ªæ•°æ®æ–‡ä»¶ï¼Œå¼€å§‹è®­ç»ƒ..."

# åˆ›å»ºå¿…è¦ç›®å½•
mkdir -p models
mkdir -p llama_factory_configs

# è®­ç»ƒæ¯ä¸ªæ¨¡å‹
for data_entry in "${AVAILABLE_DATA[@]}"; do
    IFS=':' read -r cot_type data_file <<< "$data_entry"
    
    echo ""
    echo "=========================================="
    echo "è®­ç»ƒ $cot_type CoT æ¨¡å‹"
    echo "=========================================="
    echo "æ•°æ®æ–‡ä»¶: $data_file"
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    output_dir="$(pwd)/models/qwen_${cot_type}_cot_lora"
    cutoff_len=1024
    if [[ "$cot_type" == "very_long" || "$cot_type" == "ultra_long" ]]; then
        cutoff_len=2048
    fi
    
    cat > "llama_factory_configs/qwen_${cot_type}_sft_config.yaml" << EOF
### model
model_name_or_path: $BASE_MODEL
model_revision: main

### method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.1

### dataset
dataset: tictactoe_${cot_type}_cot
dataset_dir: $(pwd)/llama_factory_configs
template: qwen
cutoff_len: $cutoff_len
max_samples: 100
overwrite_cache: true
preprocessing_num_workers: 16

### output
output_dir: $output_dir
logging_steps: 5
save_steps: 50
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
learning_rate: 5.0e-4
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

### eval
val_size: 0.1
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 50
EOF

    echo "ğŸ“ é…ç½®æ–‡ä»¶å·²åˆ›å»º"
    echo "ğŸš€ å¼€å§‹è®­ç»ƒ $cot_type æ¨¡å‹..."
    
    # æ‰§è¡Œè®­ç»ƒ
    if llamafactory-cli train "llama_factory_configs/qwen_${cot_type}_sft_config.yaml"; then
        echo "âœ… $cot_type æ¨¡å‹è®­ç»ƒå®Œæˆ!"
        echo "   æ¨¡å‹ä¿å­˜åœ¨: $output_dir"
    else
        echo "âŒ $cot_type æ¨¡å‹è®­ç»ƒå¤±è´¥!"
    fi
    
done

echo ""
echo "=========================================="
echo "æ‰¹é‡è®­ç»ƒå®Œæˆ!"
echo "=========================================="
echo "å·²è®­ç»ƒçš„æ¨¡å‹ï¼š"
ls -la models/ | grep qwen_.*_cot_lora
echo ""
echo "ä¸‹ä¸€æ­¥å¯ä»¥è¿è¡Œè¯„ä¼°è„šæœ¬:"
echo "python scripts/evaluate_all_cot_models.py"
